{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from depthai_sdk import OakCamera,Previews, FPSHandler, ResizeMode\n",
    "from depthai_sdk.managers import PipelineManager, PreviewManager, BlobManager, NNetManager\n",
    "import depthai as dai\n",
    "import cv2\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import blobconverter\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = (640,360)\n",
    "DET_INPUT_SIZE = (300,300)\n",
    "# model_name = \"face-detection-retail-0004\"\n",
    "model_name = \"yolov4_tiny_coco_416x416\"\n",
    "zoo_type = \"depthai\"\n",
    "blob_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dai.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for RGB Camera frame\n",
    "cam = pipeline.createColorCamera()\n",
    "cam.setPreviewSize(FRAME_SIZE[0],FRAME_SIZE[1])\n",
    "cam.setInterleaved(False)\n",
    "cam.setResolution(dai.ColorCameraProperties.SensorResolution.THE_1080_P)\n",
    "cam.setBoardSocket(dai.CameraBoardSocket.RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Mono Camera sources (Stereo Part for Depth)\n",
    "monoLeft = pipeline.createMonoCamera()\n",
    "monoLeft.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoLeft.setBoardSocket(dai.CameraBoardSocket.LEFT)\n",
    "\n",
    "monoRight = pipeline.createMonoCamera()\n",
    "monoRight.setResolution(dai.MonoCameraProperties.SensorResolution.THE_400_P)\n",
    "monoRight.setBoardSocket(dai.CameraBoardSocket.RIGHT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereo = pipeline.createStereoDepth()\n",
    "monoLeft.out.link(stereo.left)\n",
    "monoRight.out.link(stereo.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading C:\\Users\\hp\\.cache\\blobconverter\\yolov4_tiny_coco_416x416_openvino_2021.4_8shave.blob...\n",
      "[==================================================]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# using blobconverter to get the blob of the required model\n",
    "if model_name != None:\n",
    "    blob_path = blobconverter.from_zoo(\n",
    "        name=model_name,\n",
    "        shaves=8,\n",
    "        zoo_type=zoo_type\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining face detection NN node\n",
    "face_spac_det_nn = pipeline.createYoloSpatialDetectionNetwork()\n",
    "face_spac_det_nn.setConfidenceThreshold(0.75)\n",
    "face_spac_det_nn.setBlobPath(blob_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<depthai.ImageManipConfig at 0x1e585c1f3b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining face detection input config\n",
    "face_det_manip = pipeline.createImageManip()\n",
    "face_det_manip.initialConfig.setResize(DET_INPUT_SIZE[0],DET_INPUT_SIZE[1])\n",
    "face_det_manip.initialConfig.setKeepAspectRatio(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linking RGB output with ImageManip node, output of image manip node to neural netwrok input and the stereo depth output to the NN node\n",
    "cam.preview.link(face_det_manip.inputImage)\n",
    "face_det_manip.out.link(face_spac_det_nn.input)\n",
    "stereo.depth.link(face_spac_det_nn.inputDepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a stream to get the output from the camera\n",
    "x_preview_out = pipeline.createXLinkOut()\n",
    "x_preview_out.setStreamName(\"preview\")\n",
    "cam.preview.link(x_preview_out.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create detection output\n",
    "#creating a stream to get the output from the neural Network\n",
    "det_out = pipeline.createXLinkOut()\n",
    "det_out.setStreamName('det_out')\n",
    "face_spac_det_nn.out.link(det_out.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_info(frame, bbox, coordinates, status, status_color,fps):\n",
    "    #displaying bounding box\n",
    "    cv2.rectangle(frame, bbox,status_color[status],2)\n",
    "    #displaying coordinates\n",
    "    if coordinates is not None:\n",
    "        coord_x, coord_y, coord_z = coordinates\n",
    "        cv2.putText(frame, f\"X: {int(coord_x)} mm\", (bbox[0]+10,bbox[1]+20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "        cv2.putText(frame, f\"Y: {int(coord_y)} mm\", (bbox[0]+10,bbox[1]+20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "        cv2.putText(frame, f\"Z: {int(coord_z)} mm\", (bbox[0]+10,bbox[1]+20), cv2.FONT_HERSHEY_TRIPLEX, 0.5, 255)\n",
    "    #create bg for showing details\n",
    "    cv2.rectangle(frame,(5,5,175,100),(50,0,0),-1)\n",
    "    #Display authetication status on the frame\n",
    "    cv2.putText(frame,status,(20,40),cv2.FONT_HERSHEY_SIMPLEX,0.5,status_color[status])\n",
    "    #Display instructions on the frame\n",
    "    cv2.putText(frame,f'FPS: {fps:.2f}',(20,80), cv2.FONT_HERSHEY_SIMPLEX, 0.6,(255,255,255))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOme variables that will be used in the main loop\n",
    "# frame count\n",
    "frame_count = 0\n",
    "#placeholder fps value\n",
    "fps = 0\n",
    "#used to record the time when we process last frames\n",
    "prev_frame_time = 0\n",
    "\n",
    "#used to record the time at which we processed current frames\n",
    "next_frame_time = 0\n",
    "\n",
    "# set status colors\n",
    "status_color = {\n",
    "    'Object Detected':(0,255,0),\n",
    "    'No Object Detected':(0,0,255)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# # Output queue will be used to get nn data from the video frames\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# q_bbox_depth_mapping = device.getOutputQueue(name=\"bbox_depth_mapping_out\", maxSize=4, blocking=False)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     \u001b[39m# get right camera frame\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     in_cam \u001b[39m=\u001b[39m q_cam\u001b[39m.\u001b[39;49mget()\n\u001b[0;32m     19\u001b[0m     frame \u001b[39m=\u001b[39m in_cam\u001b[39m.\u001b[39mgetCvFrame()\n\u001b[0;32m     21\u001b[0m     bbox \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "# We start the pipeline and acquire video frames from the “preview” queue and get the NN outputs (detections and bounding box mapping) from the “det_out” queue.\n",
    "\n",
    "# Once we have the outputs, we display the spacial information and bounding box on the image frame.\n",
    "\n",
    "with dai.Device(pipeline) as device:\n",
    "    #output queue will be used to get the right camera frame from the outputs defined above\n",
    "    q_cam = device.getOutputQueue(name=\"preview\", maxSize=1,blocking=False)\n",
    "\n",
    "    #output queue will be used to get nn data from the video frames\n",
    "    q_det = device.getOutputQueue(name=\"det_out\", maxSize=1, blocking=False)\n",
    "\n",
    "    # # Output queue will be used to get nn data from the video frames\n",
    "    # q_bbox_depth_mapping = device.getOutputQueue(name=\"bbox_depth_mapping_out\", maxSize=4, blocking=False)\n",
    "\n",
    "    while True:\n",
    "        # get right camera frame\n",
    "        in_cam = q_cam.get()\n",
    "        frame = in_cam.getCvFrame()\n",
    "\n",
    "        bbox = None\n",
    "        coordinates = None\n",
    "\n",
    "        inDet = q_det.tryGet()\n",
    "\n",
    "        if inDet != None:\n",
    "            detections= inDet.detections\n",
    "            # if face detected\n",
    "            if len(detections) != 0:\n",
    "                detection = detections[0]\n",
    "\n",
    "                #correct bounding box\n",
    "                xmin = max(0,detection.xmin)\n",
    "                ymin = max(0,detection.ymin)\n",
    "                xmax = min(detection.xmax, 1)\n",
    "                ymax = min(detection.ymax, 1)\n",
    "\n",
    "                #calculate coordinates\n",
    "                x = int(xmin*FRAME_SIZE[0])\n",
    "                y = int(ymin*FRAME_SIZE[1])\n",
    "                w = int(xmax*FRAME_SIZE[0] - xmin*FRAME_SIZE[0])\n",
    "                h = int(ymax*FRAME_SIZE[1] - ymin*FRAME_SIZE[1])\n",
    "\n",
    "                bbox = (x,y,w,h)\n",
    "\n",
    "                # Get special coordinates\n",
    "                coord_x = detection.spatialCoordinates.x\n",
    "                coord_y = detection.spatialCoordinates.y\n",
    "                coord_z = detection.spatialCoordinates.z\n",
    "                print(coord_z)\n",
    "                coordinates = (coord_x,coord_y,coord_z)\n",
    "        # check if a face was detected in the frame\n",
    "        if bbox:\n",
    "            status = 'Object Detected'\n",
    "        else:\n",
    "            status = 'No Object Detected'\n",
    "\n",
    "        #display info on frame\n",
    "        display_info(frame, bbox, coordinates, status, status_color,fps)\n",
    "\n",
    "        #Calculate avg fps\n",
    "        if frame_count % 10==0:\n",
    "            # Time when we finish processing last 100 frames\n",
    "            new_frame_time = time.time()\n",
    "\n",
    "            #Fps will be number of frame processed in one second \n",
    "            fps = 1 / ((new_frame_time-prev_frame_time)/10)\n",
    "            prev_frame_time = new_frame_time\n",
    "\n",
    "        # Capture the key pressed\n",
    "        key_pressed = cv2.waitKey(1) & 0xff\n",
    "\n",
    "        # Stop the program if Esc key was pressed\n",
    "        if key_pressed == 27:\n",
    "            break\n",
    "\n",
    "        # Display the final frame\n",
    "        cv2.imshow(\"Face Cam\",frame)\n",
    "\n",
    "        #Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-m\", \"--model\", help=\"Provide model path for inference\",\n",
    "                    default='yolov4_tiny_coco_416x416', type=str)\n",
    "parser.add_argument(\"-c\", \"--config\", help=\"Provide config path for inference\",\n",
    "                    default='json/yolov4-tiny.json', type=str)\n",
    "args = parser.parse_args(\"--model yolov4_tiny_coco_416x416 --config D:\\Downloads\\Capstone_camera\".split())\n",
    "CONFIG_PATH = args.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'D:\\\\Downloads\\\\Capstone_camera'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     bm \u001b[39m=\u001b[39m BlobManager(zooName\u001b[39m=\u001b[39margs\u001b[39m.\u001b[39mmodel)\n\u001b[0;32m      9\u001b[0m nm \u001b[39m=\u001b[39m NNetManager(nnFamily\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYOLO\u001b[39m\u001b[39m\"\u001b[39m, inputSize\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m nm\u001b[39m.\u001b[39;49mreadConfig(CONFIG_PATH)  \u001b[39m# this will also parse the correct input size\u001b[39;00m\n\u001b[0;32m     12\u001b[0m pm \u001b[39m=\u001b[39m PipelineManager()\n\u001b[0;32m     13\u001b[0m pm\u001b[39m.\u001b[39mcreateColorCam(previewSize\u001b[39m=\u001b[39mnm\u001b[39m.\u001b[39minputSize, xout\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Downloads\\Capstone_camera\\venv\\lib\\site-packages\\depthai_sdk\\managers\\nnet_manager.py:81\u001b[0m, in \u001b[0;36mNNetManager.readConfig\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m configPath\u001b[39m.\u001b[39mexists():\n\u001b[0;32m     79\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPath \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not exist!\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(path))\n\u001b[1;32m---> 81\u001b[0m \u001b[39mwith\u001b[39;00m configPath\u001b[39m.\u001b[39;49mopen() \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(f)\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mopenvino_version\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_config:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1242\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mself\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, buffering\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1237\u001b[0m          errors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, newline\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1238\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m \u001b[39m    Open the file pointed by this path and return a file object, as\u001b[39;00m\n\u001b[0;32m   1240\u001b[0m \u001b[39m    the built-in open() function does.\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1242\u001b[0m     \u001b[39mreturn\u001b[39;00m io\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, mode, buffering, encoding, errors, newline,\n\u001b[0;32m   1243\u001b[0m                    opener\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_opener)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\pathlib.py:1110\u001b[0m, in \u001b[0;36mPath._opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1108\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_opener\u001b[39m(\u001b[39mself\u001b[39m, name, flags, mode\u001b[39m=\u001b[39m\u001b[39m0o666\u001b[39m):\n\u001b[0;32m   1109\u001b[0m     \u001b[39m# A stub for the opener argument to built-in open()\u001b[39;00m\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m, flags, mode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'D:\\\\Downloads\\\\Capstone_camera'"
     ]
    }
   ],
   "source": [
    "# create blob, NN, and preview managers\n",
    "if Path(args.model).exists():\n",
    "    # initialize blob manager with path to the blob\n",
    "    bm = BlobManager(blobPath=args.model)\n",
    "else:\n",
    "    # initialize blob manager with the name of the model otherwise\n",
    "    bm = BlobManager(zooName=args.model)\n",
    "\n",
    "nm = NNetManager(nnFamily=\"YOLO\", inputSize=4)\n",
    "nm.readConfig(CONFIG_PATH)  # this will also parse the correct input size\n",
    "\n",
    "pm = PipelineManager()\n",
    "pm.createColorCam(previewSize=nm.inputSize, xout=True)\n",
    "\n",
    "# create preview manager\n",
    "fpsHandler = FPSHandler()\n",
    "pv = PreviewManager(display=[Previews.color.name], scale={\"color\":0.33}, fpsHandler=fpsHandler)\n",
    "\n",
    "# create NN with managers\n",
    "nn = nm.createNN(pipeline=pm.pipeline, nodes=pm.nodes, source=Previews.color.name,\n",
    "                 blobPath=bm.getBlob(shaves=6, openvinoVersion=pm.pipeline.getOpenVINOVersion(), zooType=\"depthai\"))\n",
    "pm.addNn(nn)\n",
    "\n",
    "# initialize pipeline\n",
    "with dai.Device(pm.pipeline) as device:\n",
    "    # create outputs\n",
    "    pv.createQueues(device)\n",
    "    nm.createQueues(device)\n",
    "\n",
    "    nnData = []\n",
    "\n",
    "    while True:\n",
    "\n",
    "        # parse outputs\n",
    "        pv.prepareFrames()\n",
    "        inNn = nm.outputQueue.tryGet()\n",
    "\n",
    "        if inNn is not None:\n",
    "            nnData = nm.decode(inNn)\n",
    "            # count FPS\n",
    "            fpsHandler.tick(\"color\")\n",
    "\n",
    "        nm.draw(pv, nnData)\n",
    "        pv.showFrames()\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b742d12850034aa69f4947756db19645f14827ff8e8f28b152b8f5cf4b604b9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
